# -*- coding: utf-8 -*-

import streamlit as st
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tempfile

# --- UIè¨­å®š ---
st.set_page_config(page_title="My RAG Chatbot (Pinecone)", layout="wide")
st.title("ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&Aãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (Pineconeç‰ˆ)")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFã®å†…å®¹ã«ã¤ã„ã¦è³ªå•ã§ãã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰APIã‚­ãƒ¼ç­‰ã‚’è¨­å®šã—ã€PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€Œå­¦ç¿’ã‚’é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚
""")
st.info("**æ³¨æ„:** Geminiã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« (`embedding-001`) ã¯768æ¬¡å…ƒã§ã™ã€‚Pineconeã§Indexã‚’ä½œæˆã™ã‚‹éš›ã¯ã€æ¬¡å…ƒæ•°(Dimensions)ã‚’`768`ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")


# --- APIã‚­ãƒ¼ã¨è¨­å®š ---
with st.sidebar:
    st.header("APIã‚­ãƒ¼è¨­å®š")
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
    google_api_key = st.text_input("Google API Key", type="password", value=os.environ.get("GOOGLE_API_KEY", ""))
    pinecone_api_key = st.text_input("Pinecone API Key", type="password", value=os.environ.get("PINECONE_API_KEY", ""))
    pinecone_index_name = st.text_input("Pinecone Index Name", value=os.environ.get("PINECONE_INDEX_NAME", ""))

    st.header("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=['pdf'])

    # process_button
    process_button = st.button("å­¦ç¿’ã‚’é–‹å§‹", disabled=not uploaded_file)


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
# APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯å‡¦ç†ã‚’ä¸­æ–­
if not google_api_key or not pinecone_api_key or not pinecone_index_name:
    st.warning("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å…¨ã¦ã®APIã‚­ãƒ¼ã¨Indexåã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# LangChainã®ãƒ¢ãƒ‡ãƒ«ã¨VectorStoreã‚’åˆæœŸåŒ–
try:
    # Google Geminiã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)
    llm = ChatGoogleGenerativeAI(model="gemini-1.0-pro", google_api_key=google_api_key, convert_system_message_to_human=True)
    
    # Pinecone VectorStoreã‚’åˆæœŸåŒ–
    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings, pinecone_api_key=pinecone_api_key)
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯VectorStoreã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    st.stop()


# --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å­¦ç¿’å‡¦ç† ---
if process_button:
    if uploaded_file is not None:
        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­... ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚° -> ãƒ™ã‚¯ãƒˆãƒ«åŒ– -> DBæ ¼ç´"):
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦èª­ã¿è¾¼ã‚€
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name

                # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿
                loader = PyPDFLoader(tmp_file_path)
                documents = loader.load()

                # 2. ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰² (ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°)
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
                docs = text_splitter.split_documents(documents)

                # 3. ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦Pineconeã«æ ¼ç´
                vectorstore.add_documents(docs)

                st.success("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                os.remove(tmp_file_path)

            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- ãƒãƒ£ãƒƒãƒˆå‡¦ç† ---
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¨ä¼šè©±ãƒã‚§ãƒ¼ãƒ³ã‚’ç®¡ç†
if "messages" not in st.session_state:
    st.session_state.messages = []
if "conversation_chain" not in st.session_state:
    memory = ConversationBufferWindowMemory(k=5, memory_key="chat_history", return_messages=True)
    st.session_state.conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory
    )

# éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹
if prompt := st.chat_input("å­¦ç¿’ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„..."):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AIã®å›ç­”ã‚’ç”Ÿæˆ
    with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
        response = st.session_state.conversation_chain.invoke({"question": prompt})
        ai_response_text = response["answer"]

        # AIã®å›ç­”ã‚’è¡¨ç¤º
        st.session_state.messages.append({"role": "assistant", "content": ai_response_text})
        with st.chat_message("assistant"):
            st.markdown(ai_response_text)

